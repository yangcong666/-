# 基于ffmpeg的直播双边滤波美颜推流项目
## 项目介绍
作为一个目标为C++开发人员，为了更好的了解C++开发知识，并且为以后所要从事的具体方向做一点基础准备，所以这里选择了C++较为擅长的一个领域，也即音视频处理相关方向，所以选择了做一个直播推流项目。在该项目中，基本过程可以分解为**采集、处理、编码封装、推流传输**，其中，
- 采集分为音频采集和视频采集，他们有各自由qt提供多线程方式执行线程，而其中音频是qt库提供QAudioInput，视频是由opencv提供的VideoCapture，并且通过缓冲队列的方式实现数据存储（链表实现）。
- 处理过程同样要区分音频和视频，其中音频只是做了一个重采样处理，而视频则通过了一个opencv库内的双边滤波算法处理美颜效果，并且进行RGB转YUV格式转换。
- 编码封装，将音频采样数据PCM转换为acc格式，视频数据压缩为h.264格式。
- 推流是基于ffmpeg通过rtmp协议将数据上传至流媒体服务器，也即nigix_rtmpmo模块中处理。并且而已通过flash进行web拉流。
## 项目难点和重点
1、音视频同步问题：
```
由于我们并不能保证获取每一帧音视频的时间长短是固定在某一个定值的，而是会因为内部性能等原因导致时间上的差距，这样为了保证在进行音视频流推送的同步问题上，这里选通过外部时钟记时间的方法保存每一帧音视频录制时的时间戳，在通过rtmp协议进行上传音视频数据时，将时间戳作为参数处理。这样我们就能得到一个音视频时间准确的信息流数据。
在项目中，涉及到的具体实现有
- 时间戳函数--av_gettime();来自于 **#include <libavutil/time.h>** 得到的是一个微妙单位的时间戳。
- av_rescale_q(pack->pts, stime, dtime);编码和推流的timebase会有变化，编码是使用帧率进行计算的。推流时要改为使用推流时的timebase重新进行计算。
- 时间戳pts的的截取选择在录制音视频帧阶段，这样可以保证不会因为内部缓存的影响而导致时间出现问题。
- 
```
2、qt多线程处理
```
由于本身音频与视频数据采集并不是同步进行的，并且这里设置的直播方式为数据采集线程处理，推流线程处理。这样，对多核cpu的应用也能够更充分，并且实现逻辑更简单。在该程序中，选择qt线程类QThread来实现多线程方法，通过继承QThread并且重载run方法实现子线程逻辑。当然qt还有一些其他的创建线程接口方法，如movetothread，不过，对于常驻类型任务需求，run方法可以更好的利用这个处理逻辑。
这里子线程的推出则是通过主线程管理，主线程通过设置（bool）isExit变量值来使得run方法中while退出，这个退出不是即时的，需要将while中的主体程序运行完并再次判断条件时不满足退出。
```
## 涉及的知识点及问题
1、 保存音频和视频帧信息的数据格式?
```
首先，程序中通过一个XData类对象来统一管理音视频采集信息和采集时间，在真正需要具体的类型数据时，比如完成推流操作，这是会通过AVPacket结构保存数据，它保存着pts，dts，数据大小。视频还存在一个mat 数据类型。
```
2、 音频为什么要进行重采样和编码?
```
采样率的大小会决定整体的数据量大小，而人耳可听见音频是有范围的，这样就决定了不需要将采样频率设置过大浪费空间，但同时又不能设置的太小导致音频失真，目前一般采用44.1hz大小。而编码则是将原始数据转换为acc格式，这样目的也是为了进行数据压缩，也叫做有损压缩。项目实现函数avcodec_send_frame，前面需要进行初始化音频编码器。
```
3、 视频如何进行美颜处理？
```
项目中通过使用opencv库函数**bilateralFilter(*src, *des, d, d * 2, d / 2);**实现，对于双边滤波算法；项目通过XFilter作为算法接口，管理处理方法。
```
4、 视频从rgb转yuv的实现过程？
```
在视频采集过程中，会获得原始的rgb图像，在处理数据过程中，会先将rgb数据转换为yuv格式，涉及到的具体函数**sws_scale**，这样做可以节省一定空间，因为人眼对色彩不是特别铭感而对亮度则较为铭感，因为YUV分量中，UV分量对人眼来说不敏感，因此可以降低采样率。最后上传时则是处理为h.264。
```
5、设置工厂模式
设置工厂管理方法管理对象的生成，对于要生成对象类，通过get方法静态管理，这样，能够封装实际对象的创建过程，三种类型模式，简单工厂模式、工厂方法模式、抽象工厂模式。其中，由于项目便于管理，这里使用的事直接工厂模式，也即第二种，实现直接实现具体的类对象。
 

 ## 流媒体协议
 ### rtmp介绍
 - RTMP协议封包 由一个包头和一个包体组成,包头可以是4种长度的任意一种:12, 8, 4,  1 byte(s).完整的RTMP包头应该是12bytes,包含了时间戳,Head_Type,AMFSize,AMFType,StreamID信息, 8字节的包头只纪录了时间戳,Head_Type,AMFSize,AMFType, 4个字节的包头记录了时间戳,Head_Type。1个字节的包头只记录了Head_Type 。包体最大长度默认为128字节,通过chunkSize可改变包体最大长度,通常当一段AFM数据超过128字节后,超过128的部分就放到了其他的RTMP封包中,包头为一个字节。
 文章：吃透rtmp https://www.jianshu.com/p/b2144f9bbe28

## PCM编码为AAC
```cpp
av_register_all()：注册FFmpeg所有编解码器。

avformat_alloc_output_context2()：初始化输出码流的AVFormatContext。

avio_open()：打开输出文件。

av_new_stream()：创建输出码流的AVStream。

avcodec_find_encoder()：查找编码器。

avcodec_open2()：打开编码器。

avformat_write_header()：写文件头（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。

avcodec_encode_audio2()：编码音频。即将AVFrame（存储PCM采样数据）编码为AVPacket（存储AAC，MP3等格式的码流数据）。

av_write_frame()：将编码后的视频码流写入文件。

av_write_trailer()：写文件尾（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。

```

## YUV编码为H.264
```cpp
av_register_all()：注册FFmpeg所有编解码器。

avformat_alloc_output_context2()：初始化输出码流的AVFormatContext。

avio_open()：打开输出文件。

av_new_stream()：创建输出码流的AVStream。

avcodec_find_encoder()：查找编码器。

avcodec_open2()：打开编码器。

avformat_write_header()：写文件头（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。

avcodec_encode_video2()：编码一帧视频。即将AVFrame（存储YUV像素数据）编码为AVPacket（存储H.264等格式的码流数据）。

av_write_frame()：将编码后的视频码流写入文件。

flush_encoder()：输入的像素数据读取完成后调用此函数。用于输出编码器中剩余的AVPacket。

av_write_trailer()：写文件尾（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。
```

### 问题总结
 1、视频编码标准两大系统是什么？

视频编码标准有两大系统：MPEG和ITU-T，如下

视频编码标准

MPEG标准由MPEG制定

MPEG-1 | MPEG-2 | (MPEG-3) | MPEG-4 | MPEG-7 | MPEG-21

ITU-T标准由VCEG制定

H.261 | (H.262) | H.263 | H.263v2 | H.264

2、什么是音视频编码格式？什么是音视频封装格式？

常见的AVI、RMVB、MKV、ASF、WMV、MP4、3GP、FLV等文件其实只能算是一种封装标准。

一个完整的视频文件是由音频和视频2部分组成的。H264、Xvid等就是视频编码格式，MP3、AAC等就是音频编码格式。

例如：将一个Xvid视频编码文件和一个MP3视频编码文件按AVI封装标准封装以后，就得到一个AVI后缀的视频文件，这个就是我们常见的AVI视频文件了。

由于很多种视频编码文件、音频编码文件都符合AVI封装要求，则意味着即使是AVI后缀，也可能里面的具体编码格式不同。因此出现在一些设备上，同是AVI后缀文件，一些能正常播放，还有一些就无法播放。

同样的情况也存在于其他容器格式。即使RMVB、WMV等也不例外，事实上，很多封装容器对音频编码和视频编码的组合方式放的很开，如AVI还可以使用H.264+AAC组合，可以在具体使用中自己体会。尤其是MKV封装容器，基本无论什么样的组合都可以！但一般MKV用的最多的就是H.264+AAC组合，此组合文件体积最小，清晰度最高。因此网上很多MKV视频都是高清晰度的。

因此，视频转换需要设置的本质就是：A设置需要的视频编码、B设置需要的音频编码、C选择需要的容器封装。一个完整的视频转换设置都至少包括了上面3个步骤。

3、平时说的软解和硬解，具体是什么？
硬解就是硬件解码，指利用GPU来部分代替CPU进行解码，软解就是软件解码，指利用软件让CPU来进行解码。两者的具体区别如下所示：
硬解码：是将原来全部交由CPU来处理的视频数据的一部分交由GPU来做，而GPU的并行运算能力要远远高于CPU，这样可以大大的降低对CPU的负载，CPU的占用率较低了之后就可以同时运行一些其他的程序了，当然，对于较好的处理器来说，比如i5 2320，或者AMD 任何一款四核心处理器来说，硬解和软件的区别只是个人偏好问题了吧。　　
软解码：即通过软件让CPU来对视频进行解码处理；而硬解码：指不借助于CPU，而通过专用的子卡设备来独立完成视频解码任务。曾经的VCD/DVD解压卡、视频压缩卡等都隶属于硬解码这个范畴。而现如今，要完成高清解码已经不再需要额外的子卡，因为硬解码的模块已经被整合到显卡GPU的内部，所以目前的主流显卡（集显）都能够支持硬解码技术。
4、何为直播？何为点播？
直播：是一个三方交互(主播、服务器、观众)，这个交互式实时的！尽管会根据选择的协议不同而有一些延迟，但我们仍认为它直播是实时的！--->主播在本地发送音视频给服务器（推流），观众从服务器实时解码（拉流）收看收听主播发送给服务器的音视频（直播内容）。直播是不能快进的
点播：首先一定要明确的一点，点播不存在推流这一过程，你本身你的流已经早就推给服务器了，或者这么说也不对，应该是你的音视频早就上传到了服务器，观众只需要在线收看即可，由于你的音视频上传到了服务器，观众则可以通过快进，快退，调整进度条等方式进行收看！ 
5、简述推流、拉流的工作流程？
推流：在直播中，一方向服务器发送请求，向服务器推送自己正在实时直播的数据，而这些内容在推送到服务器的这一过程中是以 “流” 的形式传递的，这就是“推流”，把音视频数据以流的方式推送（或上传）到服务器的过程就是“推流”！ 推流方的音视频往往会很大，在推流的过程中首先按照 aac音频-编码 和 h264视【公众平台不能出现视频这两个字，真是坑】频-编码的标准把推过来的音视频压缩 ，然后合并成 MP4或者 FLV格式，然后根据直播的封装协议，最后传给服务器完成推流过程。 
拉流：与推流正好相反，拉流是用户从服务器获取推流方给服务器的音视频的过程，这就是“拉流”！拉流首先aac音频-解码 和 h.264视【公众平台不能出现视频这两个字，真是坑】 频-解码的内部把推过来的音视频解压缩，然后合成 MP4或者 FLV 格式，再解封装，最后到我们的客户端与观众进行交互。
6、常见的直播协议有哪些？之间有什么区别？
常见的直播协议有三种 RTMP、HLS、FLV...

7、RTMP：real time messaging protocol~实时传输协议，RTMP协议比较全能，既可以用来推送又可以用来直播，其核心理念是将大块的视频帧和音频帧“剁碎”，然后以小数据包的形式在互联网上进行传输，而且支持加密，因此隐私性相对比较理想，但拆包组包的过程比较复杂，所以在海量并发时也容易出现一些不可预期的稳定性问题。
8、FLV：FLV协议由Adobe公司主推，格式极其简单，只是在大块的视频帧和音视频头部加入一些标记头信息，由于这种极致的简洁，在延迟表现和大规模并发方面都很成熟。唯一的不足就是在手机浏览器上的支持非常有限，但是用作手机端APP直播协议却异常合适。
9、HLS：苹果原生：HTTP Live Streaming，遵循的是 HTTP 超文本传输协议，端口号8080，将视频分成5-10秒的视频小分片，然后用m3u8索引表进行管理，由于客户端下载到的视频都是5-10秒的完整数据，故视频的流畅性很好，但也同样引入了很大的延迟（HLS的一般延迟在10-30s左右）。

10、点播中常见的数据传输协议主要有哪些？
常见的点播协议：HLS，HTTP
11、FFmpeg是什么？



FFmpeg是一套用来记录和转换数字音视频，并能将其转化为流的开源计算机程序。拉流和推流离不开 FFmpeg 的帮助！



12、RTMP、HLS协议各自的默认端口号是？



RTMP端口号：1935

HLS端口号 ：8080 

13、rgb、aac、yuv格式所占空间大小
rgb的话占用3字节24位真彩色。

